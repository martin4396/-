# 第七讲 策略梯度

## Introduction

-  策略目标函数
   1. start value：从某状态开始算起直到终止状态个体获得的累积奖励。算法关心的是找到一个策略，当把个体放在这个状态s1时，执行当前策略，能够获得最大的start value。$J_1(\theta)=V^{\pi_\theta}(S_1)=E_{\pi_\theta } [V_1]$
   2. Average Value：对于连续环境条件，不存在开始状态，所以用平均价值。考虑个体在某时刻的状态分布，针对每个可能的状态计算从该时刻开始一直持续与环境交互下去能够得到的奖励，按该时刻个状态的概率分布求和。$J_{avV}(\theta)=\sum_sd^{\pi_\theta}(s)V^{\pi_\theta}(s)$
   3. average reward per time step：用每一个时间步长在各种情况下所能得到的平均奖励，也就是说一个确定的时间步长里，查看个体处于所有状态的可能性，然后每一种状态下采取所有行为能够得到的即时奖励，按概率求和。$J_{avR}(\theta)=\sum_sd^{\pi_\theta}(s)\sum_a\pi_\theta(s,a)R_s^a$

## Finite Difference Policy Gradient

-  policy gradient：令J（θ）是策略目标函数，策略梯度算法可以使得J（θ）沿着梯度上升至局部最大值。同时确定获得最大值时的参数θ：$\Delta\theta=\alpha \nabla_\theta J(\theta)$。其中，$\nabla_\theta J(\theta)$是策略梯度，α是步长参数。
   -  有限差分法计算策略梯度：当梯度函数比较难得到的时候，可以使用有限差分法，实际上就算根据导数的定义来计算。$\frac{\partial J(\theta)}{\partial\theta_k}\simeq\frac{J(\theta+\epsilon u_k)-J(\theta)}{\epsilon}$。Uk是一个单位向量，仅在第k个维度上值为1，其余维度为0
   -  Likelihood ratio：函数在某个变量处的梯度等于该处函数值与该函数的对数在此梯度的乘积：$\nabla_\theta\pi_\theta(s,a)=\pi_\theta(s,a)\nabla_\theta log\pi_\theta(s,a)$。定义score function为：$\nabla_\theta log\pi_\theta(s,a)$
   -  策略梯度定理：对于任何可微的策略$\pi_\theta(s,a)$对于任何策略的目标函数$J=J_1,J_{avR},or J_{avR}/(1-\gamma)$，策略梯度都是$\nabla_\theta J(\theta)=E_{\pi_\theta}[\nabla_\theta log\pi_\theta(s,a)Q^{\pi_\theta}(s,a)]$

## 蒙特卡洛策略梯度

针对具有完整的episode的情况，我们应用策略梯度理论，使用随机梯度上升来更新参数。对于公式里的期望，我们通过采样的形式来替代，即使用t时刻的return作为当前策略下行为价值的无偏估计。

## Actor-Critic策略梯度

1. critic：参数化行为价值函数$Q_w(s,a)\simeq Q^{\pi_\theta}(s,a)$
2. Actor：按照critic部分得到的价值引导策略函数参数θ的更新

则AC算法遵循的是一个近似的策略梯度：

$\nabla_\theta J(\theta)=E_{\pi_\theta}[\nabla_\theta log\pi_\theta(s,a)Q_w(s,a)]$

$\Delta\theta=\alpha\nabla_\theta log\pi_\theta(s,a)Q_w(s,a)$





**后面的内容还没来得及总结，将会今天之内完成**