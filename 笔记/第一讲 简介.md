# 第一讲 简介

## 引子

强化学习的特点：

-  没有监督数据，只有奖励信号
-  奖励信号不一定是实时的，可能是延后的，可能延后很多
-  时间是一个重要因素
-  当前的行为影响到后续的数据

## 基本概念

-  **奖励**（reward）：标量。用于反映agent在t时刻的表现。根据奖励假设，深度学习问题都可以变为最大化累积奖励的问题。奖励可能且通常是延迟的。

-  **序列决策**（sequential decision making）：选择一定的action来最大化未来的奖励。有时候可能会牺牲眼前的利益，来获得未来更多的奖励。

-  **个体和环境**（agent & environment）：agent是算法和环境交换信息的接口。agent从环境接收observation和reward，并且向环境输出action。而环境则接受action，给出observation和reward。

   

   -  **完全可观测的环境**（fully observable environment）：个体能够直接观测到环境状态。在这种条件下，个题对环境的观测=个体状态=环境状态。这种问题是一个MDP问题。
   -  **部分可间接观测的环境**（partially observable environment）：个体间接观测环境。在这种条件下，个体状态≠环境状态，个体必须构建他自己的状态呈现形式。例如：beliefs of environment state、recurrent neural network(RNN)

-  **历史和状态**（history & state）：history是一个observation、reward和action的一个序列。状态是所有决定将来的已有信息，是关于历史的一个函数：$S_t = f(H_t )$ 

   -  环境状态：是环境的私有呈现，包括环境用来决定下一个观测、奖励的所有数据，通常对agent不可见，也就是agent不能知道环境状态所有细节。即使有时候环境状态对agent可见，也会包含无用信息。
   -  个体状态：是个体的内部呈现，包括个体可以使用的、决定未来动作的所有信息。个题状态是强化学习算法可以利用的信息，是历史的函数。
   -  信息状态：包括历史上所有有用的信息，又称Markov状态。

-  **马尔可夫属性**（Markov property）：如果信息状态是可知的，那么历史信息都可以丢掉，只需要t时刻的状态信息就可以了。

   

## agent

### 主要组成成分

-  **policy**：个体行为的决策函数，是一个从状态到行为的mapping，可以是确定的也可以是不确定的
-  **value function**：对**未来奖励**的预测（指累积的，不是下一刻的），用来评价当前状态的好坏程度。价值函数是基于策略的，不同策略的价值函数是不一样的。
-  **model**（not necessary）：个体对环境的建模，体现了个体是如何思考环境运行机制的，个体希望模型能模拟环境与个体的交互机制。模型应该能解决两个问题：1. 预测下一个可能状态发生的概率 2. 预测可能获得的即时奖励

### 分类

-  value based：有value function，没有policy function。policy由value间接得到
-  policy based：没有value function。action直接由policy function给出，不维护一个对个状态价值的估计函数
-  actor-critic：同时包含value function和policy function
