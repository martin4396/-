# 第三讲 动态规划寻找最优策略

## introduction

-  动态规划：算法将复杂问题分解为子问题，通过求解子问题得到整个问题的解（化整为零）。在解决子问题时，其结果通常存储起来来解决后续的复杂问题。
-  能使用动态规划的问题的特性
   -  能够把问题的最优解分解为若干个小问题的最优解
   -  子问题在复杂问题重复出现，使得子问题的解可以储存并重复利用

-  MDP问题分类

   -  预测：给定MDP和策略，或者给定一个MRP，求出基于该策略的价值函数

   -  控制：给定一个MDP，求出最有价值函数和最优策略

      

## Iterative Policy Evaluation

-  目标：predict——求出价值函数

-  同步迭代：反向迭代，在每次迭代过程中，在第k+1次迭代，所有的状态s的价值用$v_k(s’)$ 来计算，并且更新$v_{k+1}(s')$的值。其中s'为s的后继。该方法最终能收敛到该MDP下对应策略的value function

-  异步迭代：在第k次迭代使用档次迭代的状态价值来更新状态价值

-  改善：在每一次迭代之后，马上根据迭代得到的价值函数**贪婪地**更新我们的策略。虽然不一定一次就能得到最好的策略，但是最终能够收敛到最佳策略。

-  **核心：反向迭代，最终收敛到价值函数**。

   

## Policy Iteration

-  目标：control——求出最优策略

-  方法：在当前策略上迭代计算价值函数，再根据价值函数**贪婪地更新策略**。如此反复多次，最终得到最优策略和最优状态价值函数。（贪婪指总是采取使得状态价值最大的行为；其实不一定每次都要更新策略，也可以等迭代到一定次数之后再更新，不过这样可能会影响收敛速度）

-  改善：不一定每次都要把最优价值函数迭代出来，**最优策略会更早的迭代出来**。所以可以设置一个$\epsilon$来比较两次迭代的价值函数平方差，或者设置最大迭代次数，或者每次迭代更新策略。

   

## Value Iteration

-  目标：control——求出最优策略

-  principle of optimality：一个最优策略可以分解为两部分1. 从状态s到下一个状态s'时采取了最优行为A 2.再s'时遵循最优策略。所以有以下定理：一个策略能够使得状态s获得最优价值，当且仅当：对于从状态s可以到达的任何状态s'，该策略能够使得状态s'的价值是最优价值。

-  Deterministic Value Iteration（确定性的价值迭代）：根据上一个定理，如果已知最终状态的位置和反推需要明确的状态间关系，那么可以认为是一个确定性的价值迭代。所以，可以把问题分解为一系列的子问题，从最终目标开始分析，逐渐回推，直至回推到所有状态。

-  Value  Iteration：从初始状态价值开始同步迭代计算，最终收敛，整个过程中没有遵循任何策略。根据**每一个**状态的最优后续状态价值来更新该状态的最佳状态价值。在value iteration时候，算法不会给出明确的策略，迭代过程期间得到的价值函数，不对应任何策略。

   

##  动态规划小结

| Problem    | Bellman Equation                                         | Algorithm                   |
| ---------- | -------------------------------------------------------- | --------------------------- |
| Prediction | Bellman Expectation Equation                             | Iterative Policy Evaluation |
| Control    | Bellman Expectation Equation + Greedy Policy Improvement | Policy Iteration            |
| Control    | Bellman Optimality Equation                              | Value Iteration             |

